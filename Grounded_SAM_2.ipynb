{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L20IqyAPpTNc"
      },
      "source": [
        "# Qualitative Evaluation with Grounded SAM 2\n",
        "\n",
        "#### Course: Deep Neural Engineering (IM1102)\n",
        "#### Group: Ellen Cordemans, Ilse Harmers & Sem Pepels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyOiYBpaqZMX"
      },
      "source": [
        "The code in this notebook is adapted from [1].\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "[1] Gallagher, J. (2024, July 31). How to Label Data with Grounded SAM 2. Roboflow Blog. Retrieved April 14, 2025, from https://blog.roboflow.com/label-data-with-grounded-sam-2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BS9AZ5jsbry"
      },
      "source": [
        "## Environment Set-Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FezfACh08ghP"
      },
      "outputs": [],
      "source": [
        "# Checking the availability of Google Colab's GPU. Note that this notebook cannot be run without a GPU backend.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0FTCCDX5bs9"
      },
      "outputs": [],
      "source": [
        "# Installing Grounded SAM 2 from the Autodistill (Python) library.\n",
        "# Note that this step can take a while (~ 2 minutes during our runs).\n",
        "!pip install -q autodistill-grounded-sam-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2YBW4wL6b_I"
      },
      "outputs": [],
      "source": [
        "# Making sure that the Google Colab environment has the right version of the Transformers library installed.\n",
        "!pip uninstall transformers\n",
        "!pip install -q transformers==4.49.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxqtOQevsWgY"
      },
      "outputs": [],
      "source": [
        "# Making a directory called 'data' where the data will be stored.\n",
        "# If the reader intends to run all cells in this notebook from scratch, but not in a Google Colab environment, then this step could be skipped\n",
        "# as long as the \"imagepath\" variable is adjusted as well. If the reader is running all cells in Google Colab without intending to adjust the\n",
        "# aforementioned variable, then the images should be uploaded to this new directory\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)\n",
        "\n",
        "%cd {HOME}\n",
        "!mkdir {HOME}/data\n",
        "%cd {HOME}/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5LUswDNsXej"
      },
      "source": [
        "## Grounded SAM 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hqF2RDJ5je-"
      },
      "outputs": [],
      "source": [
        "# Importing important models, functions and libraries.\n",
        "# Note that this step can take a while (~ 4 minutes in our runs).\n",
        "from autodistill_grounded_sam_2 import GroundedSAM2\n",
        "from autodistill.detection import CaptionOntology\n",
        "from autodistill.utils import plot\n",
        "import cv2\n",
        "import supervision as sv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1O5_kYy6RWs"
      },
      "outputs": [],
      "source": [
        "# This variable sets the path to the image that will processed by Grounded SAM 2.\n",
        "# The path can be modified if another image is to be processed instead.\n",
        "image_path = \"/content/data/house2.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCylTpMl6NqO"
      },
      "outputs": [],
      "source": [
        "# Setting up the Grounded SAM 2 model with our ontology. The ontology has the following structure: {\"prompt\": \"label\"}.\n",
        "# The prompt is given to the grounding model (Florence-2) and the results have the specified label attached to them.\n",
        "base_model = GroundedSAM2(\n",
        "\tontology=CaptionOntology(\n",
        "    \t{\n",
        "        \t\"door\": \"door\",\n",
        "          \"window\": \"window\",\n",
        "          \"front yard\": \"front yard\"\n",
        "    \t}\n",
        "\t)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzt1QXpnG6BK"
      },
      "outputs": [],
      "source": [
        "# Processing the image with Grounded SAM 2.\n",
        "results = base_model.predict(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0sFPcja_rP_"
      },
      "outputs": [],
      "source": [
        "# Setting up the labels for the label annotator in the next cell. We want to display both the label and the confidence score for each detection.\n",
        "classes = base_model.ontology.classes()\n",
        "\n",
        "labels = [\n",
        "    f\"{classes[class_id]} {confidence:0.2f}\"\n",
        "    for _, _, confidence, class_id, _, _\n",
        "    in results\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDW3LsKT-9ED"
      },
      "outputs": [],
      "source": [
        "# This line ensures that the color codes match between our Grounded SAM 2 results and our Roboflow dataset.\n",
        "color = sv.ColorPalette.from_hex(['#FE0056', '#8622FF', '#00FFCE'])\n",
        "\n",
        "# Setting up the annotators for our model's results.\n",
        "box_annotator = sv.BoxAnnotator(color=color)\n",
        "mask_annotator = sv.MaskAnnotator(color=color)\n",
        "label_annotator = sv.LabelAnnotator(color=color, text_color=sv.Color.BLACK)\n",
        "\n",
        "# Reading image.\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Annotating the model's results.\n",
        "annotated_image = mask_annotator.annotate(scene=image.copy(), detections=results)\n",
        "annotated_image = box_annotator.annotate(scene=annotated_image, detections=results)\n",
        "annotated_image = label_annotator.annotate(annotated_image, detections=results, labels=labels)\n",
        "\n",
        "# Plotting the annotated end result.\n",
        "sv.plot_image(annotated_image, size=(8, 8))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
